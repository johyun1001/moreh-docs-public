# Moreh AI Framework(MAF)

## Homepage
[https://moreh.io](https://moreh.io)

## Introduction

Moreh AI Framework는 클러스터에 장착된 다양한 종류의 가속기에서 PyTorch 혹은 TensorFlow로 작성된 애플리케이션을 수정 없이 실행시키는 분산 딥 러닝 프레임워크입니다.

### 다양한 가속기 지원

딥 러닝 애플리케이션은 본질적으로 텐서 연산의 나열이며, 각각의 텐서 연산은 (간혹 예외도 있지만) 미리 정의된 수백 종류 중 하나에 속합니다. PyTorch, TensorFlow 등 딥 러닝 프레임워크에서 NVIDIA GPU의 역할은 텐서 연산을 차례로 실행하는 것입니다. 바꿔 말해서, 이들 수백 종류의 텐서 연산만 모두 지원한다면 NVIDIA GPU가 아닌 다른 가속기도 딥 러닝에 사용할 수 있습니다.

***moDNN***은 PyTorch/TensorFlow 애플리케이션에서 사용되는 **모든** 텐서 연산을 구현한 라이브러리입니다. moDNN은 Moreh AI Framework를 이루는 소프트웨어 스택의 최하단에 위치하면서 다양한 가속기(AMD GPU, Intel GPU, NPU, FPGA 등)를 모두 ‘moDNN API로 텐서 연산을 실행할 수 있는 장치’로 추상화합니다. moDNN 상단에 위치한 다른 구성 요소들(Moreh AI Compiler를 포함해서)은 모두 하드웨어 독립적으로 구현 가능합니다.

### 대형 모델 지원

최근 들어 GPU를 수백~수천 개 이상 사용해야 겨우 학습이 가능한 GPT-3 등의 대형 모델들이 주목받기 시작하였습니다. 이러한 모델을 구현하기 위해서는 클러스터 시스템의 하드웨어 구성을 이해하고 이를 바탕으로 다음의 항목들을 고려하여야 합니다.

- 데이터 및 계산을 효율적으로 각 GPU에 분배하여, GPU의 모든 코어를 최대한 활용할 수 있으면서 동시에 GPU 메모리 크기 제한을 넘기지 않고 또 GPU 간 통신량을 최소화해야 합니다.
- 계산량과 데이터 크기 사이의 trade-off, 계산 성능과 데이터 크기 사이의 trade-off를 동시에 감안하여 개별 텐서 연산별로 최적의 구현 방식을 결정해야 합니다.
- GPU 간, 노드 간 통신을 어떤 순서로 어느 시점에 실행할지를 잘 결정하여, 정확성을 해치지 않는 선에서 최대한 계산과 통신이 동시에 일어나도록 해야 합니다.
- GPU 메모리가 모자랄 경우 메인 메모리를 적극적으로 활용하여야 하며 이 경우 어떤 데이터를 어느 시점에 메인 메모리로 옮기거나 다시 GPU 메모리로 옮길지를 일일히 결정해야 합니다.

PyTorch, TensorFlow 등 딥 러닝 프레임워크를 사용하면 이 모든 일을 애플리케이션 개발자가 직접 해 주어야 합니다. 이것은 (1) 매우 어렵고, (2) 하드웨어 구성이 달라지거나 모델 구조가 바뀔 때마다 처음부터 다시 해 주어야 하고, (3) 일부는 PyTorch/TensorFlow API만을 사용해서는 아예 구현 불가능한 것도 있습니다.

Moreh AI Framework는 (주로 Moreh AI Compiler에 의해) 앞의 모든 항목들을 자동으로 수행해 줍니다. 애플리케이션 개발자는 그냥 모델의 구조만 잘 기술하면 됩니다.

### GPU 코로케이션

반대로 계산량이 상대적으로 적은 딥 러닝 애플리케이션은 GPU를 100% 활용하지 않을 수도 있습니다. 심지어 실제 use case에서 GPU의 평균 utilization이 20%를 하회한다는 조사 결과도 있습니다. 이러한 경우 여러 애플리케이션이 GPU 하나를 공유하도록 하면 서비스 제공자 입장에서 인프라의 활용도를 대폭 향상시킬 수 있습니다.

두 애플리케이션 A, B가 GPU 하나를 공유하는 **실용적인 유일한** 방법은, GPU에 A가 필요로 하는 데이터와 B가 필요로 하는 데이터를 동시에 저장해 두고, A의 텐서 연산과 B의 텐서 연산을 번갈아 가며 실행하는 것입니다. 즉, GPU의 코어는 분할하지 않고(대신 번갈아 가며 사용하고) 메모리는 분할하여 사용하는 것입니다.

NVIDIA는 GPU의 코어와 메모리를 모두 분할하여 사용하는 여러 기술(Hyper-Q 등)을 제공합니다. 하지만 GPU 아키텍처의 한계로 인해 코어를 분할하면 계산 성능이 크게 저하될 수밖에 없습니다. 또한 딥 러닝 애플리케이션은 보통 latency보다는 throughput이 중요하고, 또 텐서 연산 하나에 걸리는 시간이 짧기 때문에, 굳이 코어를 분할하여 사용할 유인이 크지 않습니다.

Moreh AI Framework는 GPU 하나를 여러 애플리케이션이 공유하는 GPU 코로케이션 기능을 지원합니다. GPU 코로케이션에서 가장 중요한 것은, 애플리케이션이 GPU 메모리를 얼마나 사용할지를 미리 예측하는 일입니다. 우리는 IR을 만들어 개별 애플리케이션의 실행 패턴을 파악하고 이를 바탕으로 GPU 코로케이션이 가능한 애플리케이션 후보군을 선출합니다.

### 애플리케이션 수준 가상화

지금까지 설명한 모든 기술을 종합하여, Moreh AI Framework는 PyTorch/TensorFlow 애플리케이션에 NVIDIA GPU를 대체하는 가상의 ***SDA (Software-Defined Accelerator)***를 제공합니다. 사용자는 기존에 PyTorch 및 TensorFlow에서 NVIDIA GPU를 사용하던 방식 그대로 SDA를 사용할 수 있습니다. 예를 들어

- `torch.cuda.device_count()`을 호출하면 SDA의 개수가 반환됩니다.
- `torch.tensor(..., device='cuda:0')`을 호출하면 SDA에 텐서가 할당됩니다.
- `torch.nn.functional.conv2d()`을 호출하면 SDA에서 Convolution 연산이 실행됩니다.

SDA를 실제로 어떻게 구현할지는 우리가 자유롭게 선택할 수 있습니다. 예를 들어 AMD GPU를 사용할 수도, NVIDIA GPU를 사용할 수도, NPU나 FPGA를 사용할 수도 있습니다. 현재 애플리케이션이 실행 중인 머신에 장착된 가속기를 사용할 수도 있고, 네트워크로 연결된 다른 머신에 장착된 가속기를 사용할 수도 있습니다.

SDA 하나를 구현하기 위해 가속기 여러 개를 동시에 사용할 수도 있습니다. 예를 들어 개별 성능은 NVIDIA Tesla A100 GPU보다 떨어지지만 가성비가 좋은 컨슈머 GPU를 여러 개 묶어서 'A100-Compatible Accelerator'라는 이름의 SDA로 만들 수 있습니다. 혹은 GPT-3와 같은 대형 모델을 실행하기 위해 GPU 1,000개를 묶어서 SDA 하나로 만들 수 있습니다. 반대로 여러 SDA가 가속기 하나를 공유할 수도 있습니다.

SDA는 애플리케이션 수준 가상화 기술입니다. 즉, SDA는 PyTorch/TensorFlow 애플리케이션에서만 사용이 가능하며, 운영체제 및 다른 프로그램에서는 보이지 않습니다. 예를 들어 `lspci` 명령을 실행해도 SDA는 보이지 않습니다. 애플리케이션 수준 가상화를 통해 딥 러닝에 필요한 각종 가상화 기능들을 제공하면서도 동시에 다른 가상화 기술과 문제 없이 통합이 가능합니다.